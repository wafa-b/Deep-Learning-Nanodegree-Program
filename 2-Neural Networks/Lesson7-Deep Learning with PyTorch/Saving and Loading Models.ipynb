{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms\n",
    "import helper\n",
    "import fc_model\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define transform to normalize data\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,),(0.5,))])\n",
    "#Download training data\n",
    "trainset=datasets.FashionMNIST('~/.pytorch/F_MNIST_data/',download=True,train=True,transform=transform)\n",
    "#Load training data\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True)\n",
    "#Download test data\n",
    "testset=datasets.FashionMNIST('~/.pytorch/F_MNIST_data/',download=True,train=False,transform=transform)\n",
    "#Load test data\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label=next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Network\n",
    "model=fc_model.Network(784,10,[512,256,128])\n",
    "#Define criterion\n",
    "criterion=nn.NLLLoss()\n",
    "#Define Optimizer\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.728..  Test Loss: 1.015..  Test Accuracy: 0.616\n",
      "Epoch: 1/2..  Training Loss: 1.028..  Test Loss: 0.761..  Test Accuracy: 0.726\n",
      "Epoch: 1/2..  Training Loss: 0.853..  Test Loss: 0.700..  Test Accuracy: 0.723\n",
      "Epoch: 1/2..  Training Loss: 0.804..  Test Loss: 0.655..  Test Accuracy: 0.748\n",
      "Epoch: 1/2..  Training Loss: 0.746..  Test Loss: 0.638..  Test Accuracy: 0.764\n",
      "Epoch: 1/2..  Training Loss: 0.722..  Test Loss: 0.599..  Test Accuracy: 0.770\n",
      "Epoch: 1/2..  Training Loss: 0.732..  Test Loss: 0.579..  Test Accuracy: 0.787\n",
      "Epoch: 1/2..  Training Loss: 0.666..  Test Loss: 0.579..  Test Accuracy: 0.788\n",
      "Epoch: 1/2..  Training Loss: 0.643..  Test Loss: 0.572..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.634..  Test Loss: 0.540..  Test Accuracy: 0.798\n",
      "Epoch: 1/2..  Training Loss: 0.627..  Test Loss: 0.561..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.630..  Test Loss: 0.530..  Test Accuracy: 0.799\n",
      "Epoch: 1/2..  Training Loss: 0.617..  Test Loss: 0.522..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.605..  Test Loss: 0.512..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.610..  Test Loss: 0.513..  Test Accuracy: 0.810\n",
      "Epoch: 1/2..  Training Loss: 0.574..  Test Loss: 0.500..  Test Accuracy: 0.814\n",
      "Epoch: 1/2..  Training Loss: 0.559..  Test Loss: 0.532..  Test Accuracy: 0.801\n",
      "Epoch: 1/2..  Training Loss: 0.620..  Test Loss: 0.494..  Test Accuracy: 0.822\n",
      "Epoch: 1/2..  Training Loss: 0.592..  Test Loss: 0.507..  Test Accuracy: 0.814\n",
      "Epoch: 1/2..  Training Loss: 0.580..  Test Loss: 0.507..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.548..  Test Loss: 0.490..  Test Accuracy: 0.822\n",
      "Epoch: 1/2..  Training Loss: 0.599..  Test Loss: 0.498..  Test Accuracy: 0.815\n",
      "Epoch: 1/2..  Training Loss: 0.581..  Test Loss: 0.493..  Test Accuracy: 0.820\n",
      "Epoch: 2/2..  Training Loss: 0.599..  Test Loss: 0.497..  Test Accuracy: 0.813\n",
      "Epoch: 2/2..  Training Loss: 0.559..  Test Loss: 0.482..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.540..  Test Loss: 0.476..  Test Accuracy: 0.820\n",
      "Epoch: 2/2..  Training Loss: 0.516..  Test Loss: 0.470..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.597..  Test Loss: 0.479..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.558..  Test Loss: 0.488..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.540..  Test Loss: 0.477..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.565..  Test Loss: 0.466..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.503..  Test Loss: 0.507..  Test Accuracy: 0.813\n",
      "Epoch: 2/2..  Training Loss: 0.545..  Test Loss: 0.465..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.563..  Test Loss: 0.453..  Test Accuracy: 0.842\n",
      "Epoch: 2/2..  Training Loss: 0.510..  Test Loss: 0.450..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.494..  Test Loss: 0.459..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.447..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.475..  Test Loss: 0.462..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.486..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.534..  Test Loss: 0.468..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.534..  Test Loss: 0.460..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.486..  Test Loss: 0.450..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.497..  Test Loss: 0.466..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.515..  Test Loss: 0.451..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.511..  Test Loss: 0.448..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.544..  Test Loss: 0.455..  Test Accuracy: 0.830\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model,trainloader,testloader,criterion,optimizer,epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving and loading networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "The test dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print('Our model: \\n\\n',model,'\\n')\n",
    "#The parameters for PyTorch networks stored in model's state_dict\n",
    "print('The test dict keys: \\n\\n',model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dict using torch.save\n",
    "torch.save(model.state_dict(),'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "#load state dict using torch.load\n",
    "state_dict=torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load state dict to the network\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1320eb192258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Error because tensor sizes are wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "#Create Network with diffrente size\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "#Error because tensor sizes are wrong\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define checkpoint with same tensor sizes\n",
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function load_checkpoint to load checkpoints\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint=torch.load(filepath)\n",
    "    model=fc_model.Network(checkpoint['input_size'],\n",
    "                            checkpoint['output_size'],\n",
    "                            checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#load network\n",
    "model=load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
